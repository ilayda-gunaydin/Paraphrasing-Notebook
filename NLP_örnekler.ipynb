{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcC+0c1CjeueSutrpQXjky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilayda-gunaydin/Paraphrasing-Notebook/blob/main/NLP_%C3%B6rnekler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xjStTAZnL9HN",
        "outputId": "aa604016-5a08-432e-a313-d8cae424b411"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'believ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "word_stemmer = PorterStemmer()\n",
        "word_stemmer.stem('believes')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('believes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rQUakkP6Mfn8",
        "outputId": "d1c0f14d-c82c-4641-a7fa-58f17b12ead3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'belief'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"I am running in the park and eating apples\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemming Sonucu:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Lemmatization Sonucu:\")\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMn0xdx3PepB",
        "outputId": "5e6456a7-08b6-43b7-c414-39cd4c399c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming Sonucu:\n",
            "['i', 'am', 'run', 'in', 'the', 'park', 'and', 'eat', 'appl']\n",
            "Lemmatization Sonucu:\n",
            "['I', 'am', 'running', 'in', 'the', 'park', 'and', 'eating', 'apple']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"There are multiple ways we can perform tokenization on given text data. We can choose any method based on language, library and purpose of modeling.\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemming Sonucu:\")\n",
        "print(stemmed_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Lemmatization Sonucu:\")\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL8XpHJvRA6m",
        "outputId": "50f81212-197d-4ce6-b7dc-a4edee6f966b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming Sonucu:\n",
            "['there', 'are', 'multipl', 'way', 'we', 'can', 'perform', 'token', 'on', 'given', 'text', 'data', '.', 'we', 'can', 'choos', 'ani', 'method', 'base', 'on', 'languag', ',', 'librari', 'and', 'purpos', 'of', 'model', '.']\n",
            "Lemmatization Sonucu:\n",
            "['There', 'are', 'multiple', 'way', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize('won’t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOOov2qfYdX2",
        "outputId": "a011d24f-3fab-40b5-9063-e229d48ac807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['won', '’', 't']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "text = \"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne. \"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "#Named Entity Recognition (NER) işlevselliği\n",
        "entities = ne_chunk(tagged)\n",
        "\n",
        "for entity in entities:\n",
        "  if hasattr(entity, 'label'):\n",
        "    print(entity.label(), ''.join(c[0] for c in entity))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7keAIf3xU3j",
        "outputId": "6505d8c7-5ace-443c-dcb5-668413bf16c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERSON Apple\n",
            "ORGANIZATION Inc.\n",
            "PERSON SteveJobs\n",
            "PERSON SteveWozniak\n",
            "PERSON RonaldWayne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from translate import Translator\n",
        "translator = Translator(to_lang=\"fr\")\n",
        "text = \"Hello, how are you?\"\n",
        "\n",
        "translated_text = translator.translate(text)\n",
        "\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5h38QinzkB7",
        "outputId": "293362a0-136b-45fc-be7e-39b6cc7f85d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour, comment allez-vous ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autocorrect import Speller\n",
        "spell = Speller(lang='en')\n",
        "text = \"Hello, how ar you?\"\n",
        "corrected_text = ''.join([spell(word) for word in text.split()])\n",
        "print(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aahk1npa-Cqk",
        "outputId": "f73a1a40-a622-491d-f880-1580ef49191e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello,howaryou?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spacy part-of-speech\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the small English pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process a text\n",
        "doc = nlp(\"She ate the pizza\")\n",
        "\n",
        "# Iterate over the tokens\n",
        "for token in doc:\n",
        "    # Print the text and the predicted part-of-speech tag\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWB4hB0Xr0fa",
        "outputId": "928308eb-e2c1-4646-9ee3-8fc34675b57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She PRON\n",
            "ate VERB\n",
            "the DET\n",
            "pizza NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spacy part-of-speech\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = token.text\n",
        "    token_pos = token.pos_\n",
        "    token_dep = token.dep_\n",
        "    # This is for formatting only\n",
        "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ix-J-dUwA_T",
        "outputId": "aece6846-6ab4-45af-9da8-c624bb0185c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It          PRON      nsubj     \n",
            "’s          VERB      ccomp     \n",
            "official    NOUN      acomp     \n",
            ":           PUNCT     punct     \n",
            "Apple       PROPN     nsubj     \n",
            "is          AUX       ROOT      \n",
            "the         DET       det       \n",
            "first       ADJ       amod      \n",
            "U.S.        PROPN     nmod      \n",
            "public      ADJ       amod      \n",
            "company     NOUN      attr      \n",
            "to          PART      aux       \n",
            "reach       VERB      relcl     \n",
            "a           DET       det       \n",
            "$           SYM       quantmod  \n",
            "1           NUM       compound  \n",
            "trillion    NUM       nummod    \n",
            "market      NOUN      compound  \n",
            "value       NOUN      dobj      \n"
          ]
        }
      ]
    }
  ]
}